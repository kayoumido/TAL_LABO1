{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAL Labo 1d : application au français\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Cette dernière partie (1d) du Labo1 reprend les instructions précédentes et les applique à des textes en français.  Il s'agit d'une part de gérer correctement l'encodage de l'alphabet avec les caractères spécifiques au français, et d'autre part de voir comment les outils NLTK gèrent une langue différente de l'anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données proposées**\n",
    "\n",
    "* http://www.gutenberg.org/files/19040/19040-8.txt : livre en hongrois, ISO-8859-2 (latin2)\n",
    "* http://www.gutenberg.org/files/41211/41211-8.txt : livre en français, ISO-8859-1 (latin1)\n",
    "* http://www.gutenberg.org/files/28049/28049-0.txt : livre en polonais, UTF-8 (en Python noté 'utf-8' avec tiret) \n",
    "\n",
    "Tout d'abord, enregistrez ces fichiers depuis les URLs indiquées sur votre ordinateur (une seule consultation de chaque URL), avec l'encodage correct indiqué ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('19040-8.txt'):\n",
    "    os.system(\"wget http://www.gutenberg.org/files/19040/19040-8.txt\")\n",
    "if not os.path.isfile('41211-8.txt'):\n",
    "    os.system(\"wget http://www.gutenberg.org/files/41211/41211-8.txt\")\n",
    "if not os.path.isfile('28049-0.txt'):\n",
    "    os.system(\"wget http://www.gutenberg.org/files/28049/28049-0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant `open(filename, encoding='XXX', errors='replace')`, lisez dans une variable de type chaîne le contenu de chaque fichier, en essayant pour chacun des trois encodages (ou jeux de caractères) indiqués ci-dessus (latin1, latin2, ou utf-8).  Affichez un fragment de texte (100 à 200 caractères) et observez les différences, et si la lecture semble correcte ou non (vous aurez à remplir un tableau systématique plus bas).  Que se passe-t-il si vous n'indiquez aucun encodage lors de la lecture ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====  19040-8.txt\n",
      "The Project Gutenberg EBook of Tak�ts S�ndor Szalai Bark�czy Krisztina\n",
      "1671-1724 cz�m� k�nyv�nek ismertet�se, by Angyal D�vid\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost n\n",
      "====  19040-8.txt latin1\n",
      "The Project Gutenberg EBook of Takáts Sándor Szalai Barkóczy Krisztina\n",
      "1671-1724 czímû könyvének ismertetése, by Angyal Dávid\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost n\n",
      "====  19040-8.txt latin2\n",
      "The Project Gutenberg EBook of Takáts Sándor Szalai Barkóczy Krisztina\n",
      "1671-1724 czímű könyvének ismertetése, by Angyal Dávid\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost n\n",
      "====  19040-8.txt utf-8\n",
      "The Project Gutenberg EBook of Tak�ts S�ndor Szalai Bark�czy Krisztina\n",
      "1671-1724 cz�m� k�nyv�nek ismertet�se, by Angyal D�vid\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost n\n",
      "====  41211-8.txt\n",
      "The Project Gutenberg EBook of La com�die humaine volume I -- Sc�nes de la\n",
      "vie priv�e tome I, by Honor� de Balzac\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictio\n",
      "====  41211-8.txt latin1\n",
      "The Project Gutenberg EBook of La comédie humaine volume I -- Scènes de la\n",
      "vie privée tome I, by Honoré de Balzac\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictio\n",
      "====  41211-8.txt latin2\n",
      "The Project Gutenberg EBook of La comédie humaine volume I -- Scčnes de la\n",
      "vie privée tome I, by Honoré de Balzac\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictio\n",
      "====  41211-8.txt utf-8\n",
      "The Project Gutenberg EBook of La com�die humaine volume I -- Sc�nes de la\n",
      "vie priv�e tome I, by Honor� de Balzac\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictio\n",
      "====  28049-0.txt\n",
      "﻿The Project Gutenberg EBook of Balady i romanse, by Adam Mickiewicz\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away\n",
      "====  28049-0.txt latin1\n",
      "ï»¿The Project Gutenberg EBook of Balady i romanse, by Adam Mickiewicz\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it aw\n",
      "====  28049-0.txt latin2\n",
      "ďťżThe Project Gutenberg EBook of Balady i romanse, by Adam Mickiewicz\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it aw\n",
      "====  28049-0.txt utf-8\n",
      "﻿The Project Gutenberg EBook of Balady i romanse, by Adam Mickiewicz\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away\n",
      "====  28049-0.txt\n",
      "m at http://dp.rastko.net (this file was\n",
      "created from images generously made available by CBN Polona\n",
      "- http://www.polona.pl)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Uwagi do wydania elektronicznego.\n",
      "\n",
      "Poprawiono kilka błędów, które wy\n",
      "====  28049-0.txt latin1\n",
      "eam at http://dp.rastko.net (this file was\n",
      "created from images generously made available by CBN Polona\n",
      "- http://www.polona.pl)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Uwagi do wydania elektronicznego.\n",
      "\n",
      "Poprawiono kilka bÅÄdÃ³w, ktÃ\n",
      "====  28049-0.txt latin2\n",
      "eam at http://dp.rastko.net (this file was\n",
      "created from images generously made available by CBN Polona\n",
      "- http://www.polona.pl)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Uwagi do wydania elektronicznego.\n",
      "\n",
      "Poprawiono kilka bĹÄdĂłw, ktĂ\n",
      "====  28049-0.txt utf-8\n",
      "m at http://dp.rastko.net (this file was\n",
      "created from images generously made available by CBN Polona\n",
      "- http://www.polona.pl)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Uwagi do wydania elektronicznego.\n",
      "\n",
      "Poprawiono kilka błędów, które wy\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "for filename in [\"19040-8.txt\", \"41211-8.txt\", \"28049-0.txt\"]:\n",
    "    data = open(filename, errors='replace').read()\n",
    "    print (\"==== \", filename)\n",
    "    print(data[:200])\n",
    "    for encoding in [\"latin1\", \"latin2\", \"utf-8\"]:\n",
    "        data = open(filename, encoding=encoding, errors='replace').read()\n",
    "        print (\"==== \", filename, encoding)\n",
    "        print(data[:200])\n",
    "for filename in [\"28049-0.txt\"]:\n",
    "    data = open(filename, errors='replace').read()\n",
    "    print (\"==== \", filename)\n",
    "    print(data[600:800])\n",
    "    for encoding in [\"latin1\", \"latin2\", \"utf-8\"]:\n",
    "        data = open(filename, encoding=encoding, errors='replace').read()\n",
    "        print (\"==== \", filename, encoding)\n",
    "        print(data[600:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez résumer vos conclusions dans le tableau suivant.  Pour chaque fichier, indiquez si la lecture faite avec les différents encodages est correcte ou non, et si non, indiquez au moins un caractère qui est erronné.\n",
    "          \n",
    "| Fichier (format) / Lecture :  | utf8  | latin1  | latin2  | none |\n",
    "| ----------------------------- | ----- | ------- | ------- | ---- |\n",
    "| book_fr (original : latin1)   | incorrect \"com�die\"    | correct      | incorrect \"Scčnes\"      | incorrect \"com�die\"   |\n",
    "| book_hu (original : latin2)   |  incorrect \"Tak�ts\"    | correct ? \"czímû\"      | correct ? \"czímű\"      | incorrect \"Tak�ts\"   |\n",
    "| book_pl (original : utf-8)    | correct    | incorrect \"bÅÄdÃ³w\"      | incorrect \"bĹÄdĂłw\"      | correct   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe-t-il si vous n'utilisez pas l'option `errors='replace'` ?  Vous pouvez réutiliser le tableau pour indiquer les réponses à la question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La lecture leve une exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe1 in position 34: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d8ebad4c9a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Please write your Python code below and execute it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"19040-8.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"41211-8.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"28049-0.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"==== \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe1 in position 34: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "for filename in [\"19040-8.txt\", \"41211-8.txt\", \"28049-0.txt\"]:\n",
    "    data = open(filename).read()\n",
    "    print (\"==== \", filename)\n",
    "    print(data[:200])\n",
    "    for encoding in [\"latin1\", \"latin2\", \"utf-8\"]:\n",
    "        data = open(filename, encoding=encoding).read()\n",
    "        print (\"==== \", filename, encoding)\n",
    "        print(data[:200])\n",
    "for filename in [\"28049-0.txt\"]:\n",
    "    data = open(filename).read()\n",
    "    print (\"==== \", filename)\n",
    "    print(data[600:800])\n",
    "    for encoding in [\"latin1\", \"latin2\", \"utf-8\"]:\n",
    "        data = open(filename, encoding=encoding).read()\n",
    "        print (\"==== \", filename, encoding)\n",
    "        print(data[600:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Écrivez maintenant chaque texte dans un fichier au format utf-8**, en supprimant comme précédemment les parties initiales et finales qui proviennent du site *Project Gutenberg* et qui ne font pas partie du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192547"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "def remove_preambule(text):\n",
    "    start_delimiter = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    end_delimiter = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "    text.find(start_delimiter)\n",
    "\n",
    "    book_start = text.find(start_delimiter)\n",
    "    book_start = text.find(\"\\n\", book_start)\n",
    "    book_end = text.find(end_delimiter)\n",
    "    cleaned_book = text[book_start:book_end].replace('\\r\\n', ' ')\n",
    "    return cleaned_book\n",
    "\n",
    "book_hu = open(\"19040-8.txt\", encoding='latin2').read()\n",
    "book_pl = open(\"28049-0.txt\", encoding='utf-8').read()\n",
    "book_fr = open(\"41211-8.txt\", encoding='latin1').read()\n",
    "book_hu = remove_preambule(book_hu)\n",
    "book_pl = remove_preambule(book_pl)\n",
    "book_fr = remove_preambule(book_fr)\n",
    "open(\"19040-8.txt.utf8\", encoding='utf-8', mode=\"w\").write(book_hu)\n",
    "open(\"28049-0.txt.utf8\", encoding='utf-8', mode=\"w\").write(book_pl)\n",
    "open(\"41211-8.txt.utf8\", encoding='utf-8', mode=\"w\").write(book_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation en phrases et en mots avec NLTK, selon la langue\n",
    "\n",
    "L'objectif de cette étape est d'observer la meilleure solution pour la segmentation en phrases et la tokenization du texte français avec les outils NLTK et l'option `language='french'`.\n",
    "\n",
    "Tout d'abord, faire `import nltk` et charger le texte français dans une variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "book = book_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation en phrases.** Téléchargez d'abord le segmenteur de phrases `Punkt` pour le français avec la commande suivante.  Appliquez ce segmenteur au texte français.  Appliquez-lui également le tokenizer par défaut de NLTK, d'abord sans l'option `language=\"french\"`, puis avec elle.  Affichez et comparez le nombre de phrases obtenues dans chacun des trois cas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer_fr = nltk.data.load(resource_url = 'tokenizers/punkt/french.pickle')\n",
    "sent_tokenizer_fr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 9557\n",
      "B: 9499\n",
      "C: 9557\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "sentencesA = sent_tokenizer_fr.tokenize(book)\n",
    "print(\"A:\", len(sentencesA))\n",
    "sentencesB = nltk.sent_tokenize(book)\n",
    "print(\"B:\", len(sentencesB))\n",
    "sentencesC = nltk.sent_tokenize(book, language=\"french\")\n",
    "print(\"C:\", len(sentencesC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les trois segmentations ci-dessus, deux génèrent le même nombre de phrases noté *N*.  Testez si pour chaque *i* compris entre 1 et *N*, les *i*èmes phrases de chaque segmentation ont la même longueur en caractères et en mots, et affichez les valeurs de *i* pour lesquelles ces valeurs sont différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "for i in range(len(sentencesA)):\n",
    "    if not len(sentencesA) == len(sentencesC):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** (segmentation de chaque phrase en mots).  Comparer le tokenizer par défaut de NLTK (avec l'option `language='french'`) avec le tokenizer basé sur des expressions régulières fourni ci-après.  \n",
    "* Leurs résultats sont-ils identiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparer les tokenizations de plusieurs phrases et indiquer quelle méthode est plus adaptée.\n",
    "* Comparez le nombre total de tokens obtenus par chaque méthode.\n",
    "* En regardant la documentation de `nltk.word_tokenize`, que pensez-vous de son adaptation au français ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comme il utilise `sent_tokenize` en interne, il est aussi bien adapté au français que ce dernier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsA = [nltk.word_tokenize(i, language=\"french\") for i in sentencesA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer suivant utilise des expressions régulières.  Il est adapté de http://fabienpoulard.info/post/2008/03/05/Tokenisation-en-mots-avec-NLTK. Il peut être invoqué avec la méthode `tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer_fr = nltk.RegexpTokenizer(r'''(?x)\n",
    "          \\d+(?:\\.\\d+)?\\s*%   # les pourcentages\n",
    "        | \\w'               # les contractions d', l', ...\n",
    "        | \\w+               # les mots pleins\n",
    "        | [^\\w\\s]           # les ponctuations\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsB = [word_tokenizer_fr.tokenize(i) for i in sentencesA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 9 première phrase tokenisée différament\n",
      "====================\n",
      "['Produced', 'by', 'Claudine', 'Corbasson', 'and', 'the', 'Online', 'Distributed', 'Proofreading', 'Team', 'at', 'http', ':', '//www.pgdp.net', '(', 'This', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'The', 'Internet', 'Archive/Canadian', 'Libraries', ')', 'Au', 'lecteur', 'Cette', 'version', 'numérisée', 'reproduit', ',', 'dans', 'son', 'intégralité', ',', 'la', 'version', 'originale', '.']\n",
      "['Produced', 'by', 'Claudine', 'Corbasson', 'and', 'the', 'Online', 'Distributed', 'Proofreading', 'Team', 'at', 'http', ':', '/', '/', 'www', '.', 'pgdp', '.', 'net', '(', 'This', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'The', 'Internet', 'Archive', '/', 'Canadian', 'Libraries', ')', 'Au', 'lecteur', 'Cette', 'version', 'numérisée', 'reproduit', ',', 'dans', 'son', 'intégralité', ',', 'la', 'version', 'originale', '.']\n",
      "====================\n",
      "['La', 'ponctuation', 'n', \"'\", 'a', 'pas', 'été', 'modifiée', 'hormis', 'quelques', 'corrections', 'mineures', '.']\n",
      "['La', 'ponctuation', \"n'\", 'a', 'pas', 'été', 'modifiée', 'hormis', 'quelques', 'corrections', 'mineures', '.']\n",
      "====================\n",
      "[\"L'orthographe\", 'a', 'été', 'conservée', '.']\n",
      "[\"L'\", 'orthographe', 'a', 'été', 'conservée', '.']\n",
      "====================\n",
      "['Le', 'petit', 'Honoré', 'ne', 'fut', 'pas', 'un', 'enfant', 'prodige', ';', 'il', \"n'annonça\", 'pas', 'prématurément', \"qu'il\", 'ferait', 'la', '_Comédie', 'humaine_', '.']\n",
      "['Le', 'petit', 'Honoré', 'ne', 'fut', 'pas', 'un', 'enfant', 'prodige', ';', 'il', \"n'\", 'annonça', 'pas', 'prématurément', 'qu', \"'\", 'il', 'ferait', 'la', '_Comédie', 'humaine_', '.']\n",
      "====================\n",
      "[\"C'était\", 'un', 'garçon', 'frais', ',', 'vermeil', ',', 'bien', 'portant', ',', 'joueur', ',', 'aux', 'yeux', 'brillants', 'et', 'doux', ',', 'mais', 'que', 'rien', 'ne', 'distinguait', 'des', 'autres', ',', 'du', 'moins', 'à', 'des', 'regards', 'peu', 'attentifs', '.']\n",
      "[\"C'\", 'était', 'un', 'garçon', 'frais', ',', 'vermeil', ',', 'bien', 'portant', ',', 'joueur', ',', 'aux', 'yeux', 'brillants', 'et', 'doux', ',', 'mais', 'que', 'rien', 'ne', 'distinguait', 'des', 'autres', ',', 'du', 'moins', 'à', 'des', 'regards', 'peu', 'attentifs', '.']\n",
      "====================\n",
      "['A', 'sept', 'ans', ',', 'au', 'sortir', \"d'un\", 'externat', 'de', 'Tours', ',', 'on', 'le', 'mit', 'au', 'collége', 'de', 'Vendôme', ',', 'tenu', 'par', 'des', 'oratoriens', ',', 'où', 'il', 'passa', 'pour', 'un', 'élève', 'très-médiocre', '.']\n",
      "['A', 'sept', 'ans', ',', 'au', 'sortir', \"d'\", 'un', 'externat', 'de', 'Tours', ',', 'on', 'le', 'mit', 'au', 'collége', 'de', 'Vendôme', ',', 'tenu', 'par', 'des', 'oratoriens', ',', 'où', 'il', 'passa', 'pour', 'un', 'élève', 'très', '-', 'médiocre', '.']\n",
      "====================\n",
      "['Dédoublant', 'sa', 'personnalité', ',', 'il', 's', \"'\", 'y', 'peint', 'comme', 'un', 'ancien', 'condisciple', 'de', 'Louis', 'Lambert', ',', 'tantôt', 'en', 'parlant', 'en', 'son', 'nom', ',', 'et', 'tantôt', 'prêtant', 'ses', 'propres', 'sentiments', 'à', 'ce', 'personnage', 'imaginaire', ',', 'mais', 'pourtant', 'très-réel', ',', \"puisqu'il\", 'est', 'une', 'sorte', \"d'objectif\", 'de', \"l'âme\", 'même', 'de', \"l'écrivain\", '.']\n",
      "['Dédoublant', 'sa', 'personnalité', ',', 'il', \"s'\", 'y', 'peint', 'comme', 'un', 'ancien', 'condisciple', 'de', 'Louis', 'Lambert', ',', 'tantôt', 'en', 'parlant', 'en', 'son', 'nom', ',', 'et', 'tantôt', 'prêtant', 'ses', 'propres', 'sentiments', 'à', 'ce', 'personnage', 'imaginaire', ',', 'mais', 'pourtant', 'très', '-', 'réel', ',', 'puisqu', \"'\", 'il', 'est', 'une', 'sorte', \"d'\", 'objectif', 'de', \"l'\", 'âme', 'même', 'de', \"l'\", 'écrivain', '.']\n",
      "====================\n",
      "['Il', 'négligeait', 'de', 'faire', 'ses', 'devoirs', ';', 'mais', ',', 'favorisé', 'par', 'la', 'complicité', 'tacite', \"d'un\", 'répétiteur', 'de', 'mathématiques', ',', 'en', 'même', 'temps', 'bibliothécaire', 'et', 'occupé', 'de', 'quelque', 'ouvrage', 'transcendental', ',', 'il', 'ne', 'prenait', 'pas', 'sa', 'leçon', 'et', 'emportait', 'les', 'livres', \"qu'il\", 'voulait', '.']\n",
      "['Il', 'négligeait', 'de', 'faire', 'ses', 'devoirs', ';', 'mais', ',', 'favorisé', 'par', 'la', 'complicité', 'tacite', \"d'\", 'un', 'répétiteur', 'de', 'mathématiques', ',', 'en', 'même', 'temps', 'bibliothécaire', 'et', 'occupé', 'de', 'quelque', 'ouvrage', 'transcendental', ',', 'il', 'ne', 'prenait', 'pas', 'sa', 'leçon', 'et', 'emportait', 'les', 'livres', 'qu', \"'\", 'il', 'voulait', '.']\n",
      "====================\n",
      "['Aussi', 'fut-il', 'bientôt', \"l'élève\", 'le', 'plus', 'puni', 'de', 'sa', 'classe', '.']\n",
      "['Aussi', 'fut', '-', 'il', 'bientôt', \"l'\", 'élève', 'le', 'plus', 'puni', 'de', 'sa', 'classe', '.']\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "n = 0\n",
    "print(\"Les 9 première phrase tokenisée différament\")\n",
    "for i in range(len(wordsA)):\n",
    "    if wordsA[i] == wordsB[i]:\n",
    "        continue\n",
    "    n += 1\n",
    "    if n == 4:\n",
    "        continue\n",
    "    print(\"=\" * 20)\n",
    "    print(wordsA[i])\n",
    "    print(wordsB[i])\n",
    "    if n >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `nltk.word_tokenize` parrait être la plus adaptée elle sépare vraiment les mots, tout en maintenant un minimum ensemble les url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mot avec nltk.word_tokenize: 226899\n",
      "nombre de mot avec les regex         : 247512\n"
     ]
    }
   ],
   "source": [
    "print(\"nombre de mot avec nltk.word_tokenize:\", sum(map(len, wordsA)))\n",
    "print(\"nombre de mot avec les regex         :\", sum(map(len, wordsB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans les parties 1b et 1c, **veuillez écrire dans un fichier texte** appelé `book_fr.utf-8.tok.txt`le texte français tokenisé, avec des espaces entre les tokens et une phrase par ligne.  Utilisez la meilleure tokenization des deux précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'book_fr.utf-8.tok.txt'\n",
    "if os.path.exists(filename): \n",
    "    os.remove(filename)\n",
    "fd = open(filename, 'a', encoding='utf8')\n",
    "for line in wordsA:\n",
    "    fd.write(' '.join(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin de la partie 1d et du Labo1\n",
    "Veuillez nettoyer autant que possible ce _notebook_, exécutez une dernière fois toutes les cellules pour obtenir les résultats demandés, et enregistrez le _notebook_.  Puis ajoutez-le dans une archive _zip_ avec les _notebook_ des parties 1b et 1c, et soumettez l'archive individuellement sur Cyberlearn (_Laboratoire 1_). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
